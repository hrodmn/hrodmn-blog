---
title: "Forest Sampling: Application of Horvitz-Thompson estimators in a two stage sample"
author: "Henry Rodman"
date: "2019-02-04"
categories: ["R"]
tags: ["statistics", "sampling", "forestry", "biometrics"]

output:
  md_document:
    variant: markdown_github
    fig_width: 7
    preserve_yaml: true
---

```{r setup, include=FALSE}
pander::panderOptions(
  'table.split.cells', 5
)
pander::panderOptions(
  'table.split.table', Inf
)

library(tidyverse)
theme_set(theme_bw())
```

Though you may not notice it, forest management is a part of your life. The air we breath, water we drink, and our homes are all in some way composed of forest resources. To secure an adequate and sustainable supply of these resources, we manage forests. Humans have been managing forests with varying degrees of success throughout our entire history and the science of forestry has been developed to help us manage forests better than those who came before us. The application of statistical survey sampling techniques to the assessment of forest resources (aka forest inventory) has been elemental to improvements in forest management practices across the globe.

In any context, bad management decisions are often a product of bad information or poorly calibrated expectations for the future. The same is true in forestry and is the reason why forest inventory is so important. Efficient and unbiased sampling techniques have long been a focus in statistics, and forest managers use these methods every day to obtain the information required to make intelligent management decisions. This post is meant to outline the application of two-stage sampling to forest inventory.

## Objectives
Let's say that I am a forest manager that needs an estimate of the volume of standing trees in a tract of forest land. I want to obtain an accurate estimate for the lowest price possible. I am willing to pay more for a more precise estimate, but do not need an estimate that is more precise than +/- 10% at 90% confidence. In statistical terms, I would like the half-width of the 90% confidence interval to be roughly 10% of the estimate of the total volume.


## Simulate the population
The forest is composed of 100 stands of forest that vary in size, most of the stands are Douglas-fir plantations. In general, we expect that older stands have greater volume stocking than young stands. We will randomly assign an age, size in acres, coefficient of variation of volume within the stand, then assign the mean cubic feet per acre for the stand.
 
```{r data}
# simulate a population of stands
stands <- tibble(id = 1:100) %>%
  mutate(
    acres = truncnorm::rtruncnorm(
      n = 100, a = 1, b = 400,
      mean = 80, sd = 30
    ),
    age = truncnorm::rtruncnorm(
      n = 100, a = 10, b = 150,
      mean = 70, sd = 20
    ),
    cv = runif(n = 100, min = 10, max = 100)
  ) %>%
  mutate(
    cuftPerAc = 3000 *
      1 / (1 + exp(-(age - 40) / 15)) +
      rnorm(n = 100, mean = 0, sd = 300),
    # cuftPerAc = 50 * age +
    #   rnorm(n = 100, mean = 0, sd = 600),
    cuftTot = cuftPerAc * acres
  )

totalAcres <- sum(stands$acres)
totalCuft <- sum(stands$cuftTot)

meanCuft <- totalCuft / totalAcres

truth <- tibble(
  source = "truth",
  totalCuft = totalCuft,
  meanCuft = meanCuft
)

stands %>%
  ggplot(aes(x = age, y = cuftPerAc)) +
  geom_point() +
  ylab(bquote("volume (" ~ ft^{3} ~ "/ac)")) +
  labs(caption = "volume over age for the forest")

```
The true population mean stocking is `r round(meanCuft)` cubic feet per acre, the population total is `r round(totalCuft)` cubic feet!

## Designing a sample
There are many ways to obtain an estimate of the total volume for the forest. The most expensive way would be to visit every tree in the forest, measuring several dimensions and estimating volume for every single tree. This would provide us with an accurate and precise estimate but would be time consuming and prohibitively expensive. We could also just guess the total based on what we know about the property. This would be very cheap today, but our estimate is likely to be inaccurate and will probably result in significant costs related to bad information down the road. Our best bet is to take a sample from the population. A simple random sample of 1/15 acre circular plots distributed across the property would be a fine way to sample the property, but we also want information about individual stands for planning purposes so we should do something else.

A double sample is an appealing option because we can obtain an accurate estimate of the population total while obtaining stand-level estimates that we will use for modeling forest growth and evaluating forest management options. We can also be more efficient in our sampling efforts by controlling the total number of stands that we must visit.

### Selecting stands
The population of stands could be sampled randomly, but since we are trying to obtain an estimate of total volume for the property, we need to make sure that we sample the stands that a) contribute the most to the total volume, and b) contribute the most variance to the population. Using the Horvitz-Thompson estimators, we can sample the stands however we want so long as we know the probability of each stand being included in the sample! This is a case of _unequal probability sampling_ and the Horvitz-Thompson estimators will be statistically efficient if the probability of inclusion for each primary sampling unit is correlated with the variable of interest. In this case, we are going to weight our sample on age x acres since we believe the oldest stands are going to have the greatest stocking, and that the largest stands have the greatest total volume.

#### Inclusion probability
Some texts will approximate $\pi_{i}$ like this:

$$
\pi_{i} = n * p_{i}
$$

where $p_{i}$ is the sampling weight for the $i$th primary sample unit. This works fine when sampling with replacement, but sampling without replacement is slightly more complicated. The probability of including the $i$th element on the $k$th draw depends on the elements that were drawn in all draws $1:k$.
The distinction between $p_{i}$ and $\pi_{i}$ is very important. Think of $\pi_{i}$ as the likelihood of including the $i$th element unconditional on the other elements selected in the sample. This can be calculated by computing the proportion of all possible samples where an element is selected but that sounds very difficult. Instead we will use the power of simulation to obtain an approximation of $\pi_{i}$ for each stand.

```{r sampleSimulation}

stands$prop <- sqrt(stands$cuftPerAc) *
  stands$acres /
  sum(sqrt(stands$cuftPerAc) * stands$acres)

simulate_sample <- function(sims, stands, nSamp) {
  bind_rows(
    lapply(1:sims,
      function(i, ...) {
        stands %>%
          sample_n(nSamp, weight = prop) %>%
          mutate(try = i)
      }
    )) %>%
    group_by(id) %>%
    summarize(pi_i_sim = n() / sims) %>%
    left_join(stands, by = "id") %>%
    mutate(
      pi_i_approx = nSamp * prop
    ) %>%
    select(id, pi_i_sim, pi_i_approx, prop)
}

sampleSim20 <- simulate_sample(
  sims = 10000,
  stands = stands,
  nSamp = 20
)

sampleSim20 %>%
  ggplot(aes(x = pi_i_approx, y = pi_i_sim)) +
  geom_point() +
  geom_abline() +
  theme_bw() +
  xlim(0, 1.1 * max(sampleSim20$pi_i_approx)) +
  ylim(0, 1.1 * max(sampleSim20$pi_i_sim)) +
  xlab(bquote("approximate" ~ pi[i])) +
  ylab(bquote("simulated" ~ pi[i]))
  
sampleSim20 %>%
  left_join(stands) %>%
  ggplot(
    aes(
      x = acres,
      y = cuftPerAc,
      color = pi_i_sim)
    ) +
  geom_point()

```

As you can see, there are subtle differences between the approximate $pi_{i}$ values and the $pi_{i}$ values generated from 10,000 simulated samples. We will use the simulated values for the rest of the analysis.

For this sample we are going to weight our sample on `age * acres` since we believe the oldest stands are going to have the greatest stocking, and that the largest stands have the greatest total volume. We have decided that we are going to sample 20 stands and install either one plot per 8 acres or 30 plots per stand, whichever is fewer.

```{r sampleStands}
acresPerPlot <- 8

sampStands <- stands %>%
  sample_n(size = 20, weight = prop) %>%
  group_by(id) %>%
  mutate(
    nPlots = max(
      2,
      min(ceiling(acres / acresPerPlot), 30)
    )
  ) %>%
  left_join(
    sampleSim20 %>%
      select(id, pi_i = pi_i_sim),
    by = "id"
  )
  
stands %>%
  mutate(
    sampled = case_when(
      id %in% sampStands$id ~ "yes",
      ! id %in% sampStands$id ~ "no"
    )
  ) %>%
  ggplot(
    aes(x = acres, y = cuftPerAc, color = sampled)
  ) +
  geom_point()
  
```

### Measure plots
In each sample stand we are installing a systematic random sample of 1/15 acre circular plots. On each plot we are estimating the cubic foot volume of the living trees. This is the second stage of our two-stage sample. We are simulating plots using a truncated normal distribution based on the true mean and coefficient of variation of each stand. We are using a truncated normal distribution with a lower limit of 0 since it is not possible to observe negative volume!
```{r samplePlots}
plotTab <- sampStands %>%
  group_by(id) %>%
  mutate(
    cuftObs = list(
      truncnorm::rtruncnorm(
        a = 0, # biological minimum
        b = 6000, # biological maximum
        n = nPlots,
        mean = cuftPerAc,
        sd = (cv / 100) * cuftPerAc
      )
    )
  ) %>%
  select(id, cuftObs) %>%
  unnest() %>%
  ungroup()

```

## Analysis
Once we are done cruising it's time to crunch the numbers!

### Stand-level estimates
We can start by summarizing the stand-level estimates. For each stand we will estimate the total volume using the mean volume per acre from the sample and the total acres of each stand. We will also compute the sample variance and estimate the total variance for each stand.
```{r standStats}
standDat <- plotTab %>%
  left_join(
    sampStands %>% select(id, acres),
    by = "id"
  ) %>%
  group_by(id, acres) %>%
  summarize(
    meanObs = mean(cuftObs),
    tHat = mean(cuftObs * acres),
    varTHat = var(cuftObs * acres) / n(),
    sdObs = sd(cuftObs),
    nPlots = n()
  )
```

### Population-level estimate
We can now combine the estimates from the primary sampling units to obtain an estimate of the population. To do this we will use the inclusion and selection probabilities to weight each sampled stand's contribution to the population total.

#### Equations
The Horvitz-Thompson estimator for the population total is:
$$
\hat{t}_{HT} = \sum_{i \in S} \displaystyle \frac{\hat{t}_{i}}{\pi_{i}}
$$
where $S$ is the sample of primary sample units from population sized $N$

The estimator for variance of the population total is:

$$
\hat{V}(\hat{t}_{HT}) = \sum_{i \in S} (1 - \pi_{i}) \displaystyle \frac{\hat{t_{i}}^{2}}{\pi_{i}^{2}}+
\sum_{i \in S} \sum_{\substack{k \in S \\ k \neq i}} \displaystyle \frac{\pi_{ik} - \pi_{i}\pi_{k}}{\pi_{ik}}
\displaystyle \frac{\hat{t}_{i}}{\pi_{i}} \frac{\hat{t}_{k}}{\pi_{k}} +
\sum_{i \in S} \displaystyle \frac{\hat{V}(\hat{t_{i}})}{\pi_{i}}
$$

where $\pi_{ik}$ is the joint inclusion probability for primary sampling units $i$ and $k$.

$\pi_{ik}$ is approximated using this equation:

$$
$\pi_{ik}$ = \pi_{i} + \pi_{k} - (1 - (1 - p_{i} - p_{k})^n)
$$

We will extend the estimate of population variance ($\hat{V}(\hat{t}_{HT})$) to obtain a standard error and 90% confidence interval for the population.

There is one component of the variance calculation where each observation must be compared to the rest of the observations in the sample. There may be a more elegant way to code that part, but for now this is what I have come up with:
```{r varianceCalcFunction}
sub_var <- function(row, data, y, pi_i, p) {
  a <- data[row, ]
  b <- data[-row, ]
  n <- nrow(data)

  x <- list()
  for (l in 1:nrow(b)) {
    c <- b[l, ]

    jp <- a[[pi_i]] + c[[pi_i]] -
      (1 - (1 - a[[p]] - c[[p]])^n)

    x[[l]] <- ((jp - a[[pi_i]] * c[[pi_i]]) /
      a[[pi_i]] * c[[pi_i]]) *
      a[[y]] * c[[y]] / jp
  }

  sum(unlist(x))
}
```

Using the equations listed above we will compute estimates of the population total and variance of the total, then convert those into estimates of cubic foot volume on a per-acre basis.
```{r populationStats}

sampleSummary <- standDat %>%
  left_join(
    sampStands %>%
      select(id, pi_i, prop),
    by = "id"
  ) %>%
  ungroup() %>%
  mutate(
    subVar = unlist(
      lapply(
        1:nrow(.),
        sub_var,
        data = .,
        y = "tHat",
        pi_i = "pi_i",
        p = "prop"
      )
    )
  ) %>%
  ungroup() %>%
  summarize(
    totalCuft = sum(tHat / pi_i),
    fpc = 1 - n() / nrow(stands),
    varTot = fpc * (
      sum(((1 - pi_i) / pi_i^2) * tHat^2) +
        sum(subVar) + sum(varTHat / pi_i)
      ),
    nObs = n()
  ) %>%
  mutate(
    seTot = sqrt(varTot / nObs),
    ci90Tot = qt(
      1 - 0.1 / 2,
      df = nObs - 1
      ) * seTot,
    meanCuft = totalCuft / totalAcres,
    seMeanCuft = seTot / totalAcres,
    ci90MeanCuft = ci90Tot / totalAcres,
    source = "estimate"
  ) %>%
  select(
    source, totalCuft, seTot, ci90Tot,
    meanCuft, seMeanCuft, ci90MeanCuft,
    nObs
  )

results <- bind_rows(sampleSummary, truth)
```

#### Results
Since we know the true population mean, we are able to compare the estimate from our sample of 20 stands to the truth!
```{r sampleEstimates}
results %>%
  ggplot(aes(x = source, y = meanCuft)) +
  geom_point() +
  geom_errorbar(
    aes(
      ymax = meanCuft + ci90MeanCuft,
      ymin = meanCuft - ci90MeanCuft
    )
  ) +
  ylim(0, max(standDat$meanObs) * 1.5) +
  ylab(bquote("volume (" ~ ft^{3} ~ "/ac)"))
```

```{r simulate}

simulate_HT <- function(sims, stands, nStands,
                        acresPerPlot) {
  
  sampleSim <- simulate_sample(
    sims = 10000,
    stands = stands,
    nSamp = nStands
  )
  
  out <- bind_rows(
    lapply(
      1:sims,
      function(...) {
        sampStands <- stands %>%
          sample_n(
            size = nStands,
            weight = prop,
            replace = FALSE
          ) %>%
          group_by(id) %>%
          mutate(
            nPlots = max(
              2,
              min(ceiling(acres / acresPerPlot), 30)
            )
          ) %>%
          left_join(
            sampleSim %>%
              select(id, pi_i = pi_i_sim),
              by = "id"
            ) %>%
          ungroup()

        # stage 2: sample plots
        plotTab <- sampStands %>%
          group_by(id) %>%
          mutate(
            cuftObs = list(
              truncnorm::rtruncnorm(
                a = 0, # biological minimum
                b = 6000, # biological maximum
                n = nPlots,
                mean = cuftPerAc,
                sd = (cv / 100) * cuftPerAc
              )
            )
          ) %>%
          select(id, cuftObs) %>%
          unnest() %>%
          ungroup()

        standDat <- plotTab %>%
          left_join(
            sampStands %>%
              select(id, acres),
            by = "id"
          ) %>%
          group_by(id, acres) %>%
          summarize(
            meanObs = mean(cuftObs),
            tHat = mean(cuftObs * acres),
            varTHat = mean(acres)^2 * var(cuftObs) / n(),
            sdObs = sd(cuftObs),
            nPlots = n()
          )

        sampleSummary <- standDat %>%
          left_join(
            sampStands %>%
              select(id, pi_i, prop),
            by = "id"
          ) %>%
          ungroup() %>%
          mutate(
            subVar = unlist(
              lapply(
                1:nrow(.),
                sub_var,
                data = .,
                y = "tHat",
                pi_i = "pi_i",
                p = "prop"
              )
            )
          ) %>%
          ungroup() %>%
          summarize(
            totalCuft = sum(tHat / pi_i),
            fpc = 1 - n() / nrow(stands),
            varTot = fpc * (
              sum(((1 - pi_i) / pi_i^2) * tHat^2) +
                sum(subVar) + sum(varTHat / pi_i)
              ),
            nObs = n()
          ) %>%
          mutate(
            seTot = sqrt(varTot / nObs),
            ci90Tot = qt(
              1 - 0.1 / 2,
              df = nObs - 1
              ) * seTot,
            meanCuft = totalCuft / totalAcres,
            seMeanCuft = seTot / totalAcres,
            ci90MeanCuft = ci90Tot / totalAcres,
            source = "estimate"
          ) %>%
          select(
            source, totalCuft, seTot, ci90Tot,
            meanCuft, seMeanCuft, ci90MeanCuft,
            nObs
          )
        sampleSummary
      }
    )
  )
  
  out
}

simFrame <- bind_rows(
  map(
    .x = c(10, 20, 30, 40, 50, 60),
    .f = simulate_HT,
    stands = stands,
    sims = 100,
    acresPerPlot = acresPerPlot
  )
)

simStats <- simFrame %>%
  mutate(
    meanInCI = ifelse(
      truth$meanCuft <= meanCuft + ci90MeanCuft &
        truth$meanCuft >= meanCuft - ci90MeanCuft,
      TRUE,
      FALSE
    )
  ) %>%
  group_by(source, nObs) %>%
  summarize(
    meanCuft = mean(meanCuft),
    ci90MeanCuft = mean(ci90MeanCuft),
    propMatchCI = mean(meanInCI)
  )
```

```{r simulationPlot1}
ggplot(data = simFrame, aes(x = meanCuft)) +
  geom_histogram() +
  theme_bw() +
  facet_wrap(~nObs) +
  xlim(0, max(simFrame$meanCuft)) +
  geom_vline(
    xintercept = truth$meanCuft,
    color = "blue"
  ) +
  geom_vline(
    data = simStats,
    aes(xintercept = meanCuft),
    color = "red"
  )
```

```{r simulationPlot2}
simFrame %>%
  group_by(nObs) %>%
  # sample_n(20) %>%
  mutate(sim = row_number()) %>%
  ggplot(aes(x = meanCuft, y = sim)) +
  geom_errorbarh(
    aes(
      xmin = meanCuft - ci90MeanCuft,
      xmax = meanCuft + ci90MeanCuft
    ),
    alpha = 0.8,
    color = "orange"
  ) +
  geom_point(size = 1, alpha = 0.8) +
  geom_vline(
    xintercept = truth$meanCuft, color = "green"
  ) +
  facet_wrap(~ nObs) +
  xlim(
    0,
    max(simFrame$meanCuft) +
      max(simFrame$ci90MeanCuft)
  ) +
  theme_bw()
```

```{r simulationTable}
outTable <- simStats %>%
  bind_rows(truth %>% select(source, meanCuft)) %>%
  rename(
    "# sampled stands" = nObs,
    "average sample cu. ft. per/ac" = meanCuft,
    "90% CI" = ci90MeanCuft,
    "proportion of CIs containing true mean" = propMatchCI
  )
pander::pander(
  outTable,
  missing = "-",
  emphasize.strong.rows = which(outTable$source == "truth")
)
```
